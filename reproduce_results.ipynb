{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 2\n",
    "\n",
    "- Deliverable 2 will be a NER (Named entity recognition system).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the data\n",
    "\n",
    "url = https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus\n",
    "\n",
    "\n",
    "Essential info about entities:\n",
    "\n",
    "```\n",
    "geo = Geographical Entity\n",
    "org = Organization\n",
    "per = Person\n",
    "gpe = Geopolitical Entity\n",
    "tim = Time indicator\n",
    "art = Artifact\n",
    "eve = Event\n",
    "nat = Natural Phenomenon\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T15:43:11.016579Z",
     "start_time": "2020-05-05T15:43:11.012243Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "import skseq\n",
    "from skseq.sequences import sequence\n",
    "from skseq.sequences.sequence import Sequence\n",
    "import csv\n",
    "import os.path\n",
    "import scipy\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import skseq.sequences.structured_perceptron as spc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_list_train = pickle.load(open('data/sequence_list_train', 'rb'))\n",
    "sequence_list_validation = pickle.load(open('data/sequence_list_validation', 'rb'))\n",
    "sequence_list_test = pickle.load(open('data/sequence_list_test', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Perceptron- simple FM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Weights of Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = pickle.load(open('data/sp_01', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10. ,  1.5,  1.9, ...,  0. ,  3.6,  1. ])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 25940)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.get_num_states(), sp.get_num_observations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_corpus(sequences, sequences_predictions):\n",
    "    \"\"\"Evaluate classification accuracy at corpus level, comparing with\n",
    "    gold standard.\"\"\"\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        pred = sequences_predictions[i]\n",
    "        for j, y_hat in enumerate(pred.y):\n",
    "            if sequence.y[j] == y_hat:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = pickle.load(open('data/pred_train_1', 'rb'))\n",
    "pred_dev = pickle.load(open('data/pred_dev_1', 'rb'))\n",
    "pred_test = pickle.load(open('data/pred_test_1', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP -  Accuracy Train: 0.959 Validation: 0.696 Test: 0.657\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and print accuracies\n",
    "eval_train = evaluate_corpus(sequence_list_train.seq_list, pred_train)\n",
    "eval_dev = evaluate_corpus(sequence_list_validation.seq_list, pred_dev)\n",
    "eval_test = evaluate_corpus(sequence_list_test.seq_list, pred_test)\n",
    "print(\"SP -  Accuracy Train: %.3f Validation: %.3f Test: %.3f\"%(eval_train,eval_dev, eval_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_sequence (true, pred):\n",
    "    true_list=[]\n",
    "    for i in range(len(true)):\n",
    "        true_list.extend(true[i].y)\n",
    "    pred_list=[]\n",
    "    for i in range(len(pred)):\n",
    "        pred_list.extend(pred[i].y)\n",
    "\n",
    "    return confusion_matrix(true_list, pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[442183,    269,    347,    614,   1270,    304,    202,     36,\n",
       "           130,    156,     11,      2,      3,      2,      0,      4,\n",
       "             0],\n",
       "       [  1008,  12834,     22,   3445,    307,    454,    120,     66,\n",
       "           519,      8,     10,      0,      2,      0,      0,      2,\n",
       "             0],\n",
       "       [  1719,     53,   8145,     92,     10,      9,      6,      0,\n",
       "             3,    153,      3,      5,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [  1025,    789,      4,   7423,    245,    295,    173,      4,\n",
       "           200,      6,      7,      8,      6,      0,      0,      0,\n",
       "             0],\n",
       "       [   688,     49,      6,    308,   7082,     36,    150,     99,\n",
       "            27,      0,      4,      3,      1,      0,      0,      1,\n",
       "             0],\n",
       "       [   519,    130,      3,    335,    109,   7206,    287,      8,\n",
       "            18,      2,      3,      1,      2,      0,      0,      0,\n",
       "             0],\n",
       "       [   273,      4,      1,     32,    381,    219,   7731,     55,\n",
       "             1,      2,      0,      0,      0,      6,      0,      0,\n",
       "             0],\n",
       "       [   194,    194,      3,     58,    639,      9,    116,   2448,\n",
       "            53,      2,      0,      0,      0,      3,      0,      1,\n",
       "             0],\n",
       "       [    40,    226,      0,    136,     22,     18,      3,      4,\n",
       "          7339,      0,      8,      0,      6,      0,      0,      0,\n",
       "             0],\n",
       "       [   809,     13,    213,     22,     35,      0,      1,      0,\n",
       "             3,   2181,      9,      1,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    53,     10,      6,     12,      1,      1,      1,      0,\n",
       "             3,      2,     80,      1,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    48,      0,      3,      2,     18,      0,      1,      6,\n",
       "             0,      3,      7,     36,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    75,      6,      1,     40,      4,      6,      0,      0,\n",
       "             7,      0,      0,      0,     53,      0,      0,      0,\n",
       "             0],\n",
       "       [    52,      1,      1,      5,     50,      2,      3,      4,\n",
       "             1,      0,      0,      0,      1,     35,      0,      0,\n",
       "             0],\n",
       "       [    75,      0,      0,      6,      0,      2,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,     16,      0,\n",
       "             1],\n",
       "       [     9,      0,      0,      0,      4,      0,      5,     20,\n",
       "            12,      0,      0,      0,      0,      0,      0,     42,\n",
       "             0],\n",
       "       [    16,      0,      0,      1,      2,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             6]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix for train dataset\n",
    "confusion_matrix_sequence(sequence_list_train.seq_list, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[180432,   3980,   3063,   8210,   7801,   8713,   3942,    887,\n",
       "          3114,    645,     21,     12,     39,     10,      9,     24,\n",
       "             5],\n",
       "       [  3176,     52,     38,    173,    359,    238,     64,      4,\n",
       "            19,      8,      1,      0,      2,      0,      0,      0,\n",
       "             0],\n",
       "       [  3043,     95,     41,    175,    524,    154,    180,     12,\n",
       "            39,     14,      0,      0,      1,      1,      0,      0,\n",
       "             0],\n",
       "       [  7390,    631,     74,    368,    171,    531,    153,     11,\n",
       "            38,     10,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [  4427,     86,     23,    146,    141,    116,     51,      4,\n",
       "            39,     10,      0,      0,      0,      0,      2,      0,\n",
       "             0],\n",
       "       [  1507,     47,     13,     48,    133,     36,     39,     18,\n",
       "            11,      4,      5,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [  3556,    143,     75,    429,    121,    382,    136,      4,\n",
       "            30,      5,      0,      0,      4,      0,      0,      1,\n",
       "             0],\n",
       "       [  3126,     60,     51,    141,    368,     93,    160,     17,\n",
       "            61,     12,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [  3038,     64,     10,    395,    133,    151,    101,      8,\n",
       "            86,      5,      0,      0,      1,      0,      0,      0,\n",
       "             0],\n",
       "       [  1393,     33,      8,     42,     79,     40,     35,      5,\n",
       "            14,      4,      0,      0,      0,      0,      0,      0,\n",
       "             2],\n",
       "       [    71,      2,      0,      1,      0,      1,      1,      0,\n",
       "             2,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    56,      6,      0,      1,      2,      0,      2,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    93,      0,      0,      5,      4,     13,      2,      0,\n",
       "             2,      1,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    45,      1,      0,      5,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    60,      2,      0,      5,      5,      6,      8,      1,\n",
       "             2,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    45,      0,      0,      2,      3,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    13,      0,      0,      0,      1,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix for validation dataset\n",
    "confusion_matrix_sequence(sequence_list_validation.seq_list, pred_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[170527,   3690,   1507,  10148,   9837,  11164,  11134,    799,\n",
       "          1901,    623,     27,      3,     54,     17,      7,      6,\n",
       "             4],\n",
       "       [  3904,    414,      9,    120,    196,    297,    117,     15,\n",
       "            12,      5,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [  3228,     73,     38,    132,    239,    326,    158,      3,\n",
       "            22,      5,      0,      0,      1,      1,      7,      0,\n",
       "             0],\n",
       "       [  3212,     86,     17,    123,    233,    210,    300,     11,\n",
       "            52,      9,      0,      0,      4,      3,      0,      0,\n",
       "             7],\n",
       "       [  7207,    214,    155,    366,    339,    822,    262,     10,\n",
       "            66,     28,      0,      0,      1,      0,      0,      0,\n",
       "             0],\n",
       "       [  3055,     51,      8,    218,    223,    335,    160,      5,\n",
       "            15,      4,      1,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [  1094,     50,      1,     56,     74,    191,    108,      7,\n",
       "             4,      1,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [  3728,    270,     49,    185,    176,    466,    129,     10,\n",
       "            52,      4,      2,      0,      0,      1,      0,      0,\n",
       "             0],\n",
       "       [  2856,     92,     11,    268,    230,    364,    344,     15,\n",
       "            44,      5,      1,      0,     10,      1,      0,      0,\n",
       "             0],\n",
       "       [  1207,     67,     19,     43,     68,    291,     87,      4,\n",
       "             6,     41,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    42,      0,      0,      1,     10,      1,      0,      0,\n",
       "             0,      1,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    41,      2,      0,      4,      3,     10,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    59,      4,      2,      5,      3,     11,      6,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    38,      0,      0,      3,      4,      3,      4,      0,\n",
       "             1,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    43,      1,      0,      1,      5,      9,      1,      1,\n",
       "             1,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    41,      0,      0,      4,      0,      2,      4,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    10,      0,      0,      1,      0,      1,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix for test dataset\n",
    "confusion_matrix_sequence(sequence_list_test.seq_list, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_accuracy (true, pred):\n",
    "    true_list=[]\n",
    "    for i in range(len(true)):\n",
    "        true_list.append(true[i].y)\n",
    "    pred_list=[]\n",
    "    for i in range(len(pred)):\n",
    "        pred_list.append(pred[i].y)\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    for i in zip(true_list, pred_list):\n",
    "\n",
    "        if list(i[0]) == list(i[1]):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5600647329764721"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "sentence_accuracy (sequence_list_train.seq_list, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016175231676495365"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validation\n",
    "sentence_accuracy (sequence_list_validation.seq_list, pred_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014096254900325298"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "sentence_accuracy (sequence_list_test.seq_list, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Perceptron- Extended FM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Extended Feature Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skseq.sequences.id_feature import IDFeatures\n",
    "from skseq.sequences.id_feature import UnicodeFeatures\n",
    "\n",
    "# ----------\n",
    "# Feature Class\n",
    "# Extracts features from a labeled corpus (only supported features are extracted\n",
    "# ----------\n",
    "class ExtendedFeatures(IDFeatures):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        IDFeatures.__init__(self, dataset) #Get all the previous init of the inherited class\n",
    "        self.additional_features = [] #If empty, use all, if 0, use none\n",
    "    \n",
    "    def add_emission_features(self, sequence, pos, y, features):\n",
    "        x = sequence.x[pos]\n",
    "        # Get tag name from ID.\n",
    "        y_name = self.dataset.y_dict.get_label_name(y)\n",
    "\n",
    "        # Get word name from ID.\n",
    "        if isinstance(x, str):\n",
    "            x_name = x\n",
    "        else:\n",
    "            x_name = self.dataset.x_dict.get_label_name(x)\n",
    "\n",
    "        word = str(x_name)\n",
    "        # Generate feature name.\n",
    "        feat_name = \"id:%s::%s\" % (word, y_name)\n",
    "        # Get feature ID from name.\n",
    "        feat_id = self.add_feature(feat_name)\n",
    "        # Append feature.\n",
    "        if feat_id != -1:\n",
    "            features.append(feat_id)\n",
    "        \n",
    "        # Suffixes\n",
    "        max_suffix = 3\n",
    "        for i in range(max_suffix):\n",
    "            if len(word) > i+1:\n",
    "                suffix = word[-(i+1):]\n",
    "                # Generate feature name.\n",
    "                feat_name = \"suffix:%s::%s\" % (suffix, y_name)\n",
    "                # Get feature ID from name.\n",
    "                feat_id = self.add_feature(feat_name)\n",
    "                # Append feature.\n",
    "                if feat_id != -1:\n",
    "                    features.append(feat_id)  \n",
    "                    \n",
    "                    \n",
    "        if len(self.additional_features) == 0 or 2 in self.additional_features:\n",
    "        #If there is a capital letter at the beggining\n",
    "            if word[0].isupper():\n",
    "                # Generate feature name.\n",
    "                feat_name = \"upper::%s\" % y_name\n",
    "                # Get feature ID from name.\n",
    "                feat_id = self.add_feature(feat_name)\n",
    "                # Append feature.\n",
    "                if feat_id != -1:\n",
    "                    features.append(feat_id)\n",
    "\n",
    "        if len(self.additional_features) == 0 or 3 in self.additional_features:\n",
    "            #Hyphen\n",
    "            if str.find(word, \"-\") != -1:\n",
    "                # Generate feature name.\n",
    "                feat_name = \"hyphen::%s\" % y_name\n",
    "                # Get feature ID from name.\n",
    "                feat_id = self.add_feature(feat_name)\n",
    "                # Append feature.\n",
    "                if feat_id != -1:\n",
    "                    features.append(feat_id)\n",
    "\n",
    "        if len(self.additional_features) == 0 or 4 in self.additional_features:\n",
    "            #If there the word ends with -ed   \n",
    "            if str.endswith(word,'ed'):\n",
    "                # Generate feature name.\n",
    "                feat_name = \"verb_ed::%s\" % y_name\n",
    "                # Get feature ID from name.\n",
    "                feat_id = self.add_feature(feat_name)\n",
    "                # Append feature.\n",
    "                if feat_id != -1:\n",
    "                    features.append(feat_id)\n",
    "\n",
    "        if len(self.additional_features) == 0 or 5 in self.additional_features:\n",
    "            #If there the word ends with -ly      \n",
    "            if str.endswith(word,'ly'):\n",
    "                # Generate feature name.\n",
    "                feat_name = \"adverb_ly::%s\" % y_name\n",
    "                # Get feature ID from name.\n",
    "                feat_id = self.add_feature(feat_name)\n",
    "                # Append feature.\n",
    "                if feat_id != -1:\n",
    "                    features.append(feat_id)\n",
    "\n",
    "\n",
    "        if len(self.additional_features) == 0 or 6 in self.additional_features:\n",
    "            #If the word ends with 'ing'\n",
    "            if str.endswith(word,'ing'):\n",
    "                # Generate feature name.\n",
    "                feat_name = \"end_ing::%s\" % y_name\n",
    "                # Get feature ID from name.\n",
    "                feat_id = self.add_feature(feat_name)\n",
    "                # Append feature.\n",
    "                if feat_id != -1:\n",
    "                    features.append(feat_id)\n",
    "                    \n",
    "        if len(self.additional_features) == 0 or 7 in self.additional_features:\n",
    "            #If the word ends with 'or'\n",
    "            if str.endswith(word,'or'):\n",
    "                # Generate feature name.\n",
    "                feat_name = \"or_suf::%s\" % y_name\n",
    "                # Get feature ID from name.\n",
    "                feat_id = self.add_feature(feat_name)\n",
    "                # Append feature.\n",
    "                if feat_id != -1:\n",
    "                    features.append(feat_id)        \n",
    "        \n",
    "        if len(self.additional_features) == 0 or 8 in self.additional_features:\n",
    "            #If the word starts with 'up'\n",
    "            if str.startswith(word,'up'):\n",
    "                # Generate feature name.\n",
    "                feat_name = \"up_pref::%s\" % y_name\n",
    "                # Get feature ID from name.\n",
    "                feat_id = self.add_feature(feat_name)\n",
    "                # Append feature.\n",
    "                if feat_id != -1:\n",
    "                    features.append(feat_id)\n",
    "\n",
    "\n",
    "        if len(self.additional_features) == 0 or 9 in self.additional_features: \n",
    "            #If the word contains a full stop \n",
    "            if str.find(word, \".\") != -1:\n",
    "                # Generate feature name.\n",
    "                feat_name = \"points::%s\" % y_name\n",
    "                # Get feature ID from name.\n",
    "                feat_id = self.add_feature(feat_name)\n",
    "                # Append feature.\n",
    "                if feat_id != -1:\n",
    "                    features.append(feat_id)\n",
    "\n",
    "        if len(self.additional_features) == 0 or 10 in self.additional_features:      \n",
    "            #If the word is 'to'\n",
    "            if word == 'to':\n",
    "                # Generate feature name.\n",
    "                feat_name = \"to_prep::%s\" % y_name\n",
    "                # Get feature ID from name.\n",
    "                feat_id = self.add_feature(feat_name)\n",
    "                # Append feature.\n",
    "                if feat_id != -1:\n",
    "                    features.append(feat_id)\n",
    "\n",
    "\n",
    "        if len(self.additional_features) == 0 or 11 in self.additional_features:\n",
    "            #If the word is 'of'\n",
    "            if word == 'of':\n",
    "                # Generate feature name.\n",
    "                feat_name = \"of_prep::%s\" % y_name\n",
    "                # Get feature ID from name.\n",
    "                feat_id = self.add_feature(feat_name)\n",
    "                # Append feature.\n",
    "                if feat_id != -1:\n",
    "                    features.append(feat_id)\n",
    "\n",
    "\n",
    "\n",
    "        if len(self.additional_features) == 0 or 12 in self.additional_features:\n",
    "            #If the word is 'from'\n",
    "            if word == 'from':\n",
    "                # Generate feature name.\n",
    "                feat_name = \"from_prep::%s\" % y_name\n",
    "                # Get feature ID from name.\n",
    "                feat_id = self.add_feature(feat_name)\n",
    "                # Append feature.\n",
    "                if feat_id != -1:\n",
    "                    features.append(feat_id)\n",
    "                    \n",
    "\n",
    "        if len(self.additional_features) == 0 or 13 in self.additional_features:\n",
    "            #If the word is 'the'\n",
    "            if word == 'the':\n",
    "                # Generate feature name.\n",
    "                feat_name = \"the_art::%s\" % y_name\n",
    "                # Get feature ID from name.\n",
    "                feat_id = self.add_feature(feat_name)\n",
    "                # Append feature.\n",
    "                if feat_id != -1:\n",
    "                    features.append(feat_id)\n",
    "\n",
    "\n",
    "\n",
    "        if len(self.additional_features) == 0 or 14 in self.additional_features:\n",
    "            #If the word is 'in'\n",
    "            if word == 'in':\n",
    "                # Generate feature name.\n",
    "                feat_name = \"in_prep::%s\" % y_name\n",
    "                # Get feature ID from name.\n",
    "                feat_id = self.add_feature(feat_name)\n",
    "                # Append feature.\n",
    "                if feat_id != -1:\n",
    "                    features.append(feat_id)\n",
    "\n",
    "\n",
    "                    \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mapper_2 = ExtendedFeatures(sequence_list_train)\n",
    "feature_mapper_2.build_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Weights of Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = pickle.load(open('data/sp_02', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the various sequences using the trained model.\n",
    "pred_train = pickle.load(open('data/pred_train_2', 'rb'))\n",
    "pred_dev = pickle.load(open('data/pred_dev_2', 'rb'))\n",
    "pred_test = pickle.load(open('data/pred_test_2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP -  Accuracy Train: 0.961 Validation: 0.696 Test: 0.622\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and print accuracies\n",
    "eval_train = evaluate_corpus(sequence_list_train.seq_list, pred_train)\n",
    "eval_dev = evaluate_corpus(sequence_list_validation.seq_list, pred_dev)\n",
    "eval_test = evaluate_corpus(sequence_list_test.seq_list, pred_test)\n",
    "print(\"SP -  Accuracy Train: %.3f Validation: %.3f Test: %.3f\"%(eval_train,eval_dev, eval_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[441987,    303,    298,    752,   1503,    179,    193,     22,\n",
       "           158,    119,      7,      6,      2,      1,      0,      3,\n",
       "             0],\n",
       "       [   634,  14805,     20,   2099,    384,    179,    106,     46,\n",
       "           514,      5,      4,      0,      1,      0,      0,      0,\n",
       "             0],\n",
       "       [  2002,     61,   7865,    101,     13,      3,      6,      0,\n",
       "             3,    139,      2,      3,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [   660,    768,      2,   7898,    352,    121,    152,      2,\n",
       "           211,      4,      3,      7,      5,      0,      0,      0,\n",
       "             0],\n",
       "       [   331,     49,      5,    185,   7708,     10,     59,     75,\n",
       "            26,      0,      3,      3,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [   390,    213,      2,    900,    357,   6403,    293,      7,\n",
       "            51,      1,      5,      1,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [   133,     12,      1,     56,   1248,    112,   7050,     74,\n",
       "            15,      1,      1,      2,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [   113,    248,      2,     56,    882,      5,     52,   2308,\n",
       "            50,      1,      0,      3,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    15,    302,      0,    113,     36,     11,      2,      5,\n",
       "          7309,      0,      7,      0,      2,      0,      0,      0,\n",
       "             0],\n",
       "       [   876,     12,    223,     24,     47,      0,      0,      0,\n",
       "            20,   2084,      0,      1,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    48,      9,      5,     22,      4,      0,      0,      0,\n",
       "             8,      1,     73,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    37,      0,      3,      3,     28,      0,      0,      5,\n",
       "             1,      2,      7,     38,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    41,      9,      0,     68,     13,      3,      0,      0,\n",
       "            11,      0,      1,      0,     46,      0,      0,      0,\n",
       "             0],\n",
       "       [    24,      1,      1,      4,    101,      0,      2,      3,\n",
       "             1,      0,      0,      1,      2,     15,      0,      0,\n",
       "             0],\n",
       "       [    69,      1,      0,      7,      1,      2,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,     19,      0,\n",
       "             1],\n",
       "       [     4,      0,      0,      0,      8,      0,      3,     19,\n",
       "            13,      0,      0,      0,      0,      0,      0,     45,\n",
       "             0],\n",
       "       [    12,      0,      0,      0,      4,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             9]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix for train dataset\n",
    "confusion_matrix_sequence(sequence_list_train.seq_list, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[180508,   6477,   3000,   9809,   9089,   5117,   2211,    686,\n",
       "          3340,    597,      7,      1,     33,      7,      5,     19,\n",
       "             1],\n",
       "       [  3181,     85,     22,    256,    405,    123,     27,      3,\n",
       "            26,      5,      0,      0,      1,      0,      0,      0,\n",
       "             0],\n",
       "       [  3093,    151,     34,    208,    569,     88,     62,      9,\n",
       "            53,     10,      0,      0,      1,      0,      0,      1,\n",
       "             0],\n",
       "       [  7332,    718,     56,    469,    174,    447,    127,      3,\n",
       "            39,     12,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [  4442,    104,     46,    194,    137,     31,     20,      2,\n",
       "            47,     19,      2,      0,      0,      0,      0,      0,\n",
       "             1],\n",
       "       [  1462,     63,     12,     66,    166,     36,     19,      7,\n",
       "            20,      3,      3,      0,      0,      0,      0,      4,\n",
       "             0],\n",
       "       [  3607,    213,     56,    484,    128,    256,     81,      4,\n",
       "            47,      5,      0,      0,      5,      0,      0,      0,\n",
       "             0],\n",
       "       [  3050,    121,     48,    178,    470,     67,     67,     12,\n",
       "            70,      5,      1,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [  2922,    187,      9,    410,    154,    146,     63,      4,\n",
       "            92,      5,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [  1362,     63,      5,     54,    105,     29,      9,      5,\n",
       "            16,      5,      0,      2,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    70,      2,      0,      1,      2,      1,      1,      0,\n",
       "             1,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    56,      5,      0,      1,      4,      0,      1,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    96,      2,      0,      9,      1,      7,      2,      0,\n",
       "             3,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    45,      1,      0,      5,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    57,      3,      1,      6,      7,      2,      5,      1,\n",
       "             4,      0,      0,      0,      0,      0,      0,      3,\n",
       "             0],\n",
       "       [    45,      1,      0,      2,      2,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    13,      0,      0,      0,      1,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix for validation dataset\n",
    "confusion_matrix_sequence(sequence_list_validation.seq_list, pred_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[161106,   7085,   1378,  14635,  14313,  13236,   6329,    542,\n",
       "          2068,    617,     17,      6,     75,     28,      5,      5,\n",
       "             3],\n",
       "       [  3716,    394,      9,    207,    392,    257,     70,     11,\n",
       "            26,      7,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [  3501,     98,     25,    158,    221,    139,     52,      1,\n",
       "            34,      3,      0,      0,      0,      0,      0,      1,\n",
       "             0],\n",
       "       [  3239,    176,     17,    213,    282,    178,     88,      7,\n",
       "            52,      8,      0,      0,      4,      2,      1,      0,\n",
       "             0],\n",
       "       [  7222,    276,    114,    475,    579,    570,    137,      6,\n",
       "            83,      7,      0,      0,      1,      0,      0,      0,\n",
       "             0],\n",
       "       [  3097,     97,     11,    285,    312,    194,     55,      5,\n",
       "            18,      0,      0,      0,      0,      1,      0,      0,\n",
       "             0],\n",
       "       [  1122,     51,      0,     87,    141,    123,     53,      3,\n",
       "             5,      1,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [  3730,    312,     33,    268,    253,    337,     60,      8,\n",
       "            67,      0,      4,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [  2824,    191,     13,    374,    351,    247,    168,     11,\n",
       "            44,      5,      0,      4,      9,      0,      0,      0,\n",
       "             0],\n",
       "       [  1298,     63,     11,     65,    104,    199,     34,      9,\n",
       "            11,     39,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    40,      0,      0,      8,      4,      1,      0,      0,\n",
       "             1,      1,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    42,      1,      0,      5,      2,      0,      9,      0,\n",
       "             1,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    62,      4,      1,      9,      7,      7,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    38,      1,      0,      3,      9,      1,      0,      0,\n",
       "             1,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    35,      1,      4,      6,      4,      4,      3,      0,\n",
       "             1,      4,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [    37,      0,      0,      4,      4,      1,      5,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0],\n",
       "       [     5,      0,      0,      2,      1,      4,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix for test dataset\n",
    "confusion_matrix_sequence(sequence_list_test.seq_list, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6023901406697373"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "sentence_accuracy (sequence_list_train.seq_list, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016933445661331085"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validation\n",
    "sentence_accuracy (sequence_list_validation.seq_list, pred_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001668195846192343"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "sentence_accuracy (sequence_list_test.seq_list, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron evaluation using the testing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The/0 programmers/0 from/0 Barcelona/0 might/0 write/0 a/0 sentence/0 without/0 a/0 spell/0 checker./0 The/0 programmers/0 from/0 Barchelona/0 cannot/0 write/0 a/0 sentence/0 without/0 a/0 spell/0 checker./0 Jack/0 London/0 went/0 to/0 Parris./0 Jack/0 London/0 went/0 to/0 Paris./0 We/0 never/0 though/0 Microsoft/0 would/0 become/0 such/0 a/0 big/0 company./0 We/0 never/0 though/0 Microsof/0 would/0 become/0 such/0 a/0 big/0 company./0 The/0 president/0 of/0 U.S.A/0 though/0 they/0 could/0 win/0 the/0 war/0 The/0 president/0 of/0 the/0 United/0 States/0 of/0 America/0 though/0 they/0 could/0 win/0 the/0 war/0 The/0 king/0 of/0 Saudi/0 Arabia/0 wanted/0 total/0 control./0 Robin/0 does/0 not/0 want/0 to/0 go/0 to/0 Saudi/0 Arabia./0 Apple/0 is/0 a/0 great/0 company./0 I/0 really/0 love/0 apples/0 and/0 oranges./0 "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = \"The programmers from Barcelona might write a sentence without a spell checker. The programmers from Barchelona cannot write a sentence without a spell checker. Jack London went to Parris. Jack London went to Paris. We never though Microsoft would become such a big company. We never though Microsof would become such a big company. The president of U.S.A though they could win the war The president of the United States of America though they could win the war The king of Saudi Arabia wanted total control. Robin does not want to go to Saudi Arabia. Apple is a great company. I really love apples and oranges.\"\n",
    "\n",
    "new_seq = skseq.sequences.sequence.Sequence(x=p.split(), y=[int(0) for w in p.split()])\n",
    "new_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0]],\n",
       " [[8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8],\n",
       "  [8]],\n",
       " [[105]],\n",
       " [[106, 2, 23, 3],\n",
       "  [36208, 62, 135, 136],\n",
       "  [867, 270, 868, 869, 870],\n",
       "  [36092, 310, 311, 36093, 3],\n",
       "  [2029, 10, 2030, 2031],\n",
       "  [21757, 2, 445, 2625],\n",
       "  [94],\n",
       "  [9623, 2, 140, 504],\n",
       "  [4258, 10, 453, 557],\n",
       "  [94],\n",
       "  [5, 6, 772],\n",
       "  [6935, 103],\n",
       "  [106, 2, 23, 3],\n",
       "  [36208, 62, 135, 136],\n",
       "  [867, 270, 868, 869, 870],\n",
       "  [310, 311, 36093, 3],\n",
       "  [10, 264],\n",
       "  [21757, 2, 445, 2625],\n",
       "  [94],\n",
       "  [9623, 2, 140, 504],\n",
       "  [4258, 10, 453, 557],\n",
       "  [94],\n",
       "  [5, 6, 772],\n",
       "  [6935, 103],\n",
       "  [253, 254, 255, 3],\n",
       "  [38, 108, 1535, 3],\n",
       "  [4543, 10, 230, 231],\n",
       "  [89, 90, 91],\n",
       "  [6935, 3, 103],\n",
       "  [253, 254, 255, 3],\n",
       "  [38, 108, 1535, 3],\n",
       "  [4543, 10, 230, 231],\n",
       "  [89, 90, 91],\n",
       "  [6935, 3, 103],\n",
       "  [1, 2, 3],\n",
       "  [4310, 13, 14, 93],\n",
       "  [6803, 146, 1038, 1039],\n",
       "  [10, 1056, 12407, 3],\n",
       "  [366, 85, 367, 368],\n",
       "  [678, 2, 574, 575],\n",
       "  [760, 146, 404, 761],\n",
       "  [94],\n",
       "  [13684, 201, 9186],\n",
       "  [6935, 103],\n",
       "  [1, 2, 3],\n",
       "  [4310, 13, 14, 93],\n",
       "  [6803, 146, 1038, 1039],\n",
       "  [20, 7092, 3],\n",
       "  [366, 85, 367, 368],\n",
       "  [678, 2, 574, 575],\n",
       "  [760, 146, 404, 761],\n",
       "  [94],\n",
       "  [13684, 201, 9186],\n",
       "  [6935, 103],\n",
       "  [106, 2, 23, 3],\n",
       "  [229, 10, 230, 231],\n",
       "  [19, 20, 21],\n",
       "  [14666, 38098, 3, 103],\n",
       "  [6803, 146, 1038, 1039],\n",
       "  [648, 33, 649, 650],\n",
       "  [1019, 85, 367, 368],\n",
       "  [4764, 38, 129],\n",
       "  [22, 2, 23, 24],\n",
       "  [846, 13, 158],\n",
       "  [106, 2, 23, 3],\n",
       "  [229, 10, 230, 231],\n",
       "  [19, 20, 21],\n",
       "  [22, 2, 23, 24],\n",
       "  [41008, 85, 86, 483, 3, 88],\n",
       "  [62, 118, 1282, 3],\n",
       "  [19, 20, 21],\n",
       "  [310, 5873, 41047, 3],\n",
       "  [6803, 146, 1038, 1039],\n",
       "  [648, 33, 649, 650],\n",
       "  [1019, 85, 367, 368],\n",
       "  [4764, 38, 129],\n",
       "  [22, 2, 23, 24],\n",
       "  [846, 13, 158],\n",
       "  [106, 2, 23, 3],\n",
       "  [5507, 201, 202, 203, 204],\n",
       "  [19, 20, 21],\n",
       "  [2931, 975, 2932, 2933, 3],\n",
       "  [310, 941, 942, 3],\n",
       "  [7547, 85, 86, 483, 88],\n",
       "  [6770, 5, 303, 526],\n",
       "  [6935, 103],\n",
       "  [38, 129, 8749, 3],\n",
       "  [782, 62, 118, 783],\n",
       "  [263, 10, 264],\n",
       "  [5025, 10, 230, 1573],\n",
       "  [89, 90, 91],\n",
       "  [833, 90],\n",
       "  [89, 90, 91],\n",
       "  [2931, 975, 2932, 2933, 3],\n",
       "  [6935, 36822, 3, 103],\n",
       "  [2, 267, 850, 3],\n",
       "  [83, 62],\n",
       "  [94],\n",
       "  [4339, 10, 238, 3764],\n",
       "  [6935, 103],\n",
       "  [6795, 3],\n",
       "  [20247, 33, 394, 816, 396],\n",
       "  [24757, 2, 391, 1433],\n",
       "  [12005, 62, 118, 448],\n",
       "  [126, 85, 127],\n",
       "  [6935, 103]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper_2.get_sequence_features(new_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(The/0 programmers/0 from/0 Barcelona/1 might/0 write/0 a/0 sentence/0 without/0 a/0 spell/5 checker./6 The/0 programmers/0 from/0 Barchelona/3 cannot/4 write/0 a/0 sentence/0 without/0 a/0 spell/5 checker./6 Jack/0 London/1 went/0 to/0 Parris./5 Jack/6 London/6 went/0 to/0 Paris./5 We/6 never/0 though/0 Microsoft/3 would/0 become/0 such/0 a/0 big/0 company./5 We/6 never/0 though/0 Microsof/0 would/0 become/0 such/0 a/0 big/5 company./6 The/0 president/0 of/0 U.S.A/3 though/0 they/0 could/0 win/0 the/0 war/0 The/0 president/0 of/0 the/0 United/1 States/7 of/7 America/7 though/0 they/0 could/0 win/0 the/0 war/0 The/0 king/0 of/0 Saudi/1 Arabia/7 wanted/0 total/0 control./5 Robin/6 does/0 not/0 want/0 to/0 go/0 to/0 Saudi/3 Arabia./4 Apple/4 is/0 a/0 great/0 company./5 I/6 really/0 love/0 apples/0 and/0 oranges./5 ,\n",
       " 3143.200000000003)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.viterbi_decode(new_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The/O programmers/O from/O Barcelona/B-geo might/O write/O a/O sentence/O without/O a/O spell/B-per checker./I-per The/O programmers/O from/O Barchelona/B-org cannot/I-org write/O a/O sentence/O without/O a/O spell/B-per checker./I-per Jack/O London/B-geo went/O to/O Parris./B-per Jack/I-per London/I-per went/O to/O Paris./B-per We/I-per never/O though/O Microsoft/B-org would/O become/O such/O a/O big/O company./B-per We/I-per never/O though/O Microsof/O would/O become/O such/O a/O big/B-per company./I-per The/O president/O of/O U.S.A/B-org though/O they/O could/O win/O the/O war/O The/O president/O of/O the/O United/B-geo States/I-geo of/I-geo America/I-geo though/O they/O could/O win/O the/O war/O The/O king/O of/O Saudi/B-geo Arabia/I-geo wanted/O total/O control./B-per Robin/I-per does/O not/O want/O to/O go/O to/O Saudi/B-org Arabia./I-org Apple/I-org is/O a/O great/O company./B-per I/I-per really/O love/O apples/O and/O oranges./B-per '"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.viterbi_decode(new_seq)[0].to_words(sequence_list_train,\n",
    "                                       only_tag_translation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_hat = pickle.load(open('data/Y_hat', 'rb'))\n",
    "Y = pickle.load(open('data/Y', 'rb'))\n",
    "Y_hat_2 = pickle.load(open('data/Y_hat_2', 'rb'))\n",
    "Y_2 = pickle.load(open('data/Y_2', 'rb'))\n",
    "Y_hat_3 = pickle.load(open('data/Y_hat_3', 'rb'))\n",
    "Y_3 = pickle.load(open('data/Y_3', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hmm_prediction (Y, Y_hat):\n",
    "    correct = 0\n",
    "    total   = 0\n",
    "    for y,y_hat in zip(Y,Y_hat):\n",
    "        for y_hat_k, y_k in zip(y,y_hat):\n",
    "            total +=1\n",
    "            if y_hat_k == y_k:\n",
    "                correct +=1\n",
    "    correct, total\n",
    "    print(\"Accuracy posterior decode data\", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy posterior decode data 0.7115939936471267\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "hmm_prediction(Y, Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy posterior decode data 0.9754079495500472\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "hmm_prediction(Y_2, Y_hat_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy posterior decode data 0.9766103547232104\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "hmm_prediction(Y_3, Y_hat_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
